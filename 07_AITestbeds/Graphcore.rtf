{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa321\partightenfactor0

\f0\b\fs48 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Output on GraphCore\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 Running the MNIST example with the default settings\
Extracting /home/kylekaba/.torch/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/kylekaba/.torch/datasets/MNIST/raw\
TrainingModelWithLoss( (model): Network( (layer1): Block( (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer2): Block( (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer3): Linear(in_features=1600, out_features=128, bias=True) (layer3_act): ReLU() (layer3_dropout): Dropout(p=0.5, inplace=False) (layer4): Linear(in_features=128, out_features=10, bias=True) (softmax): Softmax(dim=1) ) (loss): CrossEntropyLoss() ) Accuracy on test set: 98.05%\
Now, we changed the learning rate from 0.03 to 0.1\
TrainingModelWithLoss( (model): Network( (layer1): Block( (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer2): Block( (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer3): Linear(in_features=1600, out_features=128, bias=True) (layer3_act): ReLU() (layer3_dropout): Dropout(p=0.5, inplace=False) (layer4): Linear(in_features=128, out_features=10, bias=True) (softmax): Softmax(dim=1) ) (loss): CrossEntropyLoss() ) Accuracy on test set: 98.18%\
Changing the learning rate from 0.03 to 0.1 has increased the accuracy on the test set by 0.13% relative to the default hyperparameter settings.\
Now we set the learning rate back to 0.03 and we change the batch_size to 32.\
(poptorch33_env) (base) kylekaba@gc-poplar-02:~/graphcore/examples/tutorials/simple_applications/pytorch/mnist$ /opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py srun: job 20916 queued and waiting for resources srun: job 20916 has been allocated resources Epochs: 0%| | 0/10 [00:00<?,[20:12:27.555] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10 | 0/150 [00:00<?, ?it/s] Graph compilation: 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 100/100 [00:00<00:00] Epochs: 100%|\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 10/10 [01:34<00:00, 9.40s/it] Graph compilation: 100%|\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 100/100 [00:00<00:00]\
77TrainingModelWithLoss(00:03<00:00, 41.54it/s]00<00:00] (model): Network( (layer1): Block( (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer2): Block( (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (relu): ReLU() ) (layer3): Linear(in_features=1600, out_features=128, bias=True) (layer3_act): ReLU() (layer3_dropout): Dropout(p=0.5, inplace=False) (layer4): Linear(in_features=128, out_features=10, bias=True) (softmax): Softmax(dim=1) ) (loss): CrossEntropyLoss() ) Accuracy on test set: 98.26%\
Changing the batch size from 8 to 32 has resulted in a 0.19% increase in accuracy on the test set from the default hyperparameter settings.\
}